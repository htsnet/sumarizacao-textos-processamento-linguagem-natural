{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Udemy Sumarizacao com Algoritmo de Luhn.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOURQit6VmyoIKYr1JvxeUD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/htsnet/sumarizacao-textos-processamento-linguagem-natural/blob/main/Udemy_Sumarizacao_com_Algoritmo_de_Luhn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdjWL2KJFWYO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gcUk7_RFgYa"
      },
      "source": [
        "# Solução para o exercício - Lematização Luhn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKnODsPxFqhf"
      },
      "source": [
        "import spacy\r\n",
        "import re\r\n",
        "import json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8Wf1pjuGqYQ",
        "outputId": "d7971582-b1ce-4c13-8a75-a494f7f4eef9"
      },
      "source": [
        "!python -m spacy download pt\r\n",
        "\r\n",
        "pln = spacy.load('pt')\r\n",
        "pln"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[38;5;1m✘ Server error (503)\u001b[0m\n",
            "Couldn't fetch available shortcuts. Please find a model for your spaCy\n",
            "installation (v2.2.4), and download it manually. For more details, see the\n",
            "documentation: https://spacy.io/usage/models\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.lang.pt.Portuguese at 0x7f5ad1b4f240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfMTpay0GRUw"
      },
      "source": [
        "def preprocessamento_lematizacao(texto):\r\n",
        "  texto = texto.lower()\r\n",
        "  texto = re.sub(r\" +\", ' ', texto)\r\n",
        "\r\n",
        "  documento = pln(texto)\r\n",
        "  tokens = []\r\n",
        "  for token in documento:\r\n",
        "    tokens.append(token.lemma_)\r\n",
        "\r\n",
        "  tokens = [palavra for palavra in tokens if palavra not in stopwords and palavra not in string.ponctuation]  \r\n",
        "  texto_formatado = ' '.join([str(elemento) for elemento in tokens if not elemento.isdigit()])\r\n",
        "\r\n",
        "  return texto_formatado\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYbT0Z8DGP6T"
      },
      "source": [
        "def sumarizar_lematizacao(texto, top_n_palavras, distancia, quantidade_sentencas):\r\n",
        "  sentencas_originais = [sentenca for sentenca in nltk.sent_tokenize(texto)]\r\n",
        "  sentencas_formatadas = [preprocessamento_lematizacao(sentenca_original) for sentenca_original in sentencas_originais]\r\n",
        "  palavras = [palavras.lower() for sentenca in sentencas_formatadas for palavra in nltk.tokenize.word_tokenize(sentenca)]\r\n",
        "  frequencia = nltk.FreqDist(palavras)\r\n",
        "  top_n_palavras = [palavras[0] for palavra in frequencia.most_common(top_n_palavras)]\r\n",
        "  notas_sentencas = calcula_nota_sentencas(sentencas_formatadas, top_n_palavras, distancia)\r\n",
        "  melhores_sentencas = heapq.nlargest(quantidade_sentencas, notas_sentencas)\r\n",
        "  melhores_sentencas = [sentencas_originais[i] for (nota, i) in melhores_sentencas]\r\n",
        "\r\n",
        "  return sentencas_originais, melhores_sentencas, notas_sentencas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "Ko8aXKttJC-Z",
        "outputId": "bdeea868-958f-4798-f1eb-326953fddd76"
      },
      "source": [
        "artigos_blog = json.loads(open('/content/feed_iaexpert.json').read())\r\n",
        "artigos_blog[0][conteudo]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-21a234bc0c4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0martigos_blog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/feed_iaexpert.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0martigos_blog\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconteudo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/feed_iaexpert.json'"
          ]
        }
      ]
    }
  ]
}